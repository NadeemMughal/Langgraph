{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlcRb9+I+qcyRmPXxMDJ5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadeemMughal/Langgraph_Panaversity_Urdu/blob/main/Langgraph_Chatbot_Booking_Appointment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNe6_QrpFHIc",
        "outputId": "bfc209ad-0614-4b70-b2fe-9107cfef5d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.62-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.3.29)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.8.3)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (2.10.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.51-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.2.2)\n",
            "Downloading langgraph-0.2.62-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.2/138.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\n",
            "Downloading langgraph_sdk-0.1.51-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, dataclasses-json, langgraph-checkpoint, langgraph, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.14 langgraph-0.2.62 langgraph-checkpoint-2.0.9 langgraph-sdk-0.1.51 marshmallow-3.25.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_google_genai langchain langgraph langchain_core langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "hrXv6qNkFQio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiV49LIfGWFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-1.5-pro', api_key= gemini_api_key, max_output_tokens=1024)\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHle6FtNFmew",
        "outputId": "0f7aa956-76af-4c77-d4c6-009efe5bcbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='models/gemini-1.5-pro', google_api_key=SecretStr('**********'), max_output_tokens=1024, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e88d145faf0>, default_metadata=())"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, START, StateGraph, MessagesState\n",
        "from langgraph.prebuilt import ToolNode"
      ],
      "metadata": {
        "id": "_apK6JslGJFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(MessagesState):\n",
        "  pass"
      ],
      "metadata": {
        "id": "H3f3G_DgHa1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"\n",
        "    Adds two numbers together.\n",
        "\n",
        "    Args:\n",
        "        a (float): The first number.\n",
        "        b (float): The second number.\n",
        "\n",
        "    Returns:\n",
        "        float: The sum of the two numbers.\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def subtract(a: float, b: float) -> float:\n",
        "    \"\"\"\n",
        "    Subtracts the second number from the first.\n",
        "\n",
        "    Args:\n",
        "        a (float): The first number.\n",
        "        b (float): The second number.\n",
        "\n",
        "    Returns:\n",
        "        float: The result of the subtraction.\n",
        "    \"\"\"\n",
        "    return a - b\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"\n",
        "    Multiplies two numbers together.\n",
        "\n",
        "    Args:\n",
        "        a (float): The first number.\n",
        "        b (float): The second number.\n",
        "\n",
        "    Returns:\n",
        "        float: The product of the two numbers.\n",
        "    \"\"\"\n",
        "    return a * b\n"
      ],
      "metadata": {
        "id": "S2wrGtIKHay_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bind the functions for use in langgraph\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "tools = [add, subtract, multiply]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "\n",
        "#bound_tools = bind_tools[add, subtract, multiply]\n",
        "\n",
        "# Verify that the tools are bound\n",
        "#for tool_name in bound_tools:\n",
        " #   print(f\"Tool '{tool_name}' has been successfully bound.\")\n"
      ],
      "metadata": {
        "id": "gA4fcpZIHawn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AnyMessage,SystemMessage\n",
        "sys_msg = SystemMessage(content= 'You are a helpful assistant of user queries.')\n",
        "\n",
        "\n",
        "def chatbot(state: MessagesState) -> MessagesState:\n",
        "  return {'messages': [llm_with_tools.invoke([sys_msg] +state['messages'])] }"
      ],
      "metadata": {
        "id": "i5FOwa0BdWIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6QrvcZJe_lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "\n",
        "# Build Grafrom langgraph.prebuilt import tools_conditionph\n",
        "builder: StateGraph = StateGraph(state_schema=MessagesState)\n",
        "builder.add_node(\"ChatBot-LLM\", llm_with_tools)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6khZBSqHarl",
        "outputId": "48ad843a-225e-41b1-daa7-5986d5968ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e88d11c55d0>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import tools_condition\n",
        "# Logic'\n",
        "builder.add_edge(START, \"ChatBot-LLM\")\n",
        "builder.add_conditional_edges(\"ChatBot-LLM\", tools_condition)\n",
        "\n",
        "builder.add_edge(\"tools\", \"ChatBot-LLM\")\n",
        "builder.add_edge(\"ChatBot-LLM\", END)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFA5VrSnHapH",
        "outputId": "45cef393-0144-4f1f-c038-d1f765281058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e88d11c55d0>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fooi-Zp3o6FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph.state import CompiledStateGraph\n",
        "graph: CompiledStateGraph = builder.compile()"
      ],
      "metadata": {
        "id": "RjbeEtAQHamR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "tKgJYlhAmt-n",
        "outputId": "4fd81b5a-b62c-48ba-cf9a-87b42821144f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "SJkVd1DBqNLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Chat state to track messages\n",
        "# class ChatState:\n",
        "#     def __init__(self):\n",
        "#         self.history = []  # Store history of interactions\n",
        "#         self.messages = []  # Initialize messages list\n",
        "\n",
        "#     def add_to_history(self, question, answer):\n",
        "#         self.history.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "#     def get_history(self):\n",
        "#         return self.history\n",
        "\n",
        "\n",
        "# # Chatbot function\n",
        "# def chatbot(state: ChatState) -> ChatState:\n",
        "#     user_question = state.messages[-1][\"content\"]\n",
        "#     response = llm_with_tools.invoke([sys_msg] + state.messages)\n",
        "#     answer = response[\"content\"]\n",
        "\n",
        "#     # Update history\n",
        "#     state.add_to_history(user_question, answer)\n",
        "\n",
        "#     # Add the response to messages with the 'assistant' role\n",
        "#     state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "#     return state\n"
      ],
      "metadata": {
        "id": "bOK57s3hqbo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Simulate chatbot interaction\n",
        "# state = ChatState()\n",
        "# while True:\n",
        "#     user_input = input(\"You: \")\n",
        "#     if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "#         print(\"Exiting the chatbot. Here's your history:\")\n",
        "#         for interaction in state.get_history():\n",
        "#             print(f\"Q: {interaction['question']}\\nA: {interaction['answer']}\")\n",
        "#         break\n",
        "\n",
        "#     # Add user message to state with the 'user' role\n",
        "#     state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "#     # Get chatbot response\n",
        "#     state = chatbot(state)\n",
        "\n",
        "#     # Display response\n",
        "#     print(f\"Bot: {state.messages[-1]['content']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "y9Pld7RIn-MY",
        "outputId": "feca03ea-969b-4294-87f0-a4b3bb663a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: What is 2+2?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'AIMessage' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-b1a835052d27>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Get chatbot response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Display response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-113-78b3f2b7973e>\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_with_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys_msg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Update history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'AIMessage' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Chat state to track messages\n",
        "# class ChatState:\n",
        "#     def __init__(self):\n",
        "#         self.history = []  # Store history of interactions\n",
        "#         self.messages = []  # Initialize messages list\n",
        "\n",
        "#     def add_to_history(self, question, answer):\n",
        "#         self.history.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "#     def get_history(self):\n",
        "#         return self.history\n",
        "\n",
        "\n",
        "# # Chatbot function\n",
        "# def chatbot(state: ChatState) -> ChatState:\n",
        "#     user_question = state.messages[-1][\"content\"]\n",
        "\n",
        "#     # Use the LLM with tools to get a response\n",
        "#     response = llm_with_tools.invoke([sys_msg] + state.messages)\n",
        "#     answer = response.content  # Extract the content from AIMessage\n",
        "\n",
        "#     # Update history\n",
        "#     state.add_to_history(user_question, answer)\n",
        "\n",
        "#     # Add the response to messages with the 'assistant' role\n",
        "#     state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "#     return state\n",
        "\n",
        "\n",
        "# Simulate chatbot interaction\n",
        "state = ChatState()\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Exiting the chatbot. Here's your history:\")\n",
        "        for interaction in state.get_history():\n",
        "            print(f\"Q: {interaction['question']}\\nA: {interaction['answer']}\")\n",
        "        break\n",
        "\n",
        "    # Add user message to state with the 'user' role\n",
        "    state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Get chatbot response\n",
        "    state = chatbot(MessagesState)\n",
        "\n",
        "    # Display response\n",
        "    print(f\"Bot: {state.messages[-1]['content']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "bDdH-Sd8qOwU",
        "outputId": "f957905d-63e2-4bea-9263-cf42ae60f07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: hi\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can only concatenate list (not \"types.GenericAlias\") to list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-ec719d2a9169>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Get chatbot response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMessagesState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Display response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-8c20e872a6c9>\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMessagesState\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMessagesState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'messages'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mllm_with_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys_msg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'messages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"types.GenericAlias\") to list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install from PyPI\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEqL2IM1dPAt",
        "outputId": "d4cca66d-71fe-45f3-ac88-c496c90274c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.59.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "# OPENAI_API_KEY = userdata.get('OPEN_API_KEY')"
      ],
      "metadata": {
        "id": "z5B0vCVGcSyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "\n",
        "# # Initialize the OpenAI client with your API key\n",
        "# openai.api_key = OPENAI_API_KEY # Make sure OPENAI_API_KEY is defined and contains your key\n",
        "\n",
        "# # Now you can create the completion\n",
        "# # Use openai.chat.completions.create instead of openai.ChatCompletion.create\n",
        "# response = openai.chat.completions.create(\n",
        "#     model=\"gpt-4o-mini\",  # Use \"gpt-4\" or \"gpt-4-turbo\" based on your requirements\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#         {\"role\": \"user\", \"content\": \"Hello, how can I use OpenAI models instead of Google Gemini?\"}\n",
        "#     ],\n",
        "#     max_tokens=1024\n",
        "# )\n",
        "\n",
        "# # Print the response\n",
        "# print(response.choices[0].message.content) # Access content using . instead of []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "9dc6DUrpcmpZ",
        "outputId": "5c819981-7b60-4618-a72c-cef7ee70019a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-263-a5663f98b8c5>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Now you can create the completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use openai.chat.completions.create instead of openai.ChatCompletion.create\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m response = openai.chat.completions.create( \n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Use \"gpt-4\" or \"gpt-4-turbo\" based on your requirements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    857\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1278\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         )\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Langgraph-llm-With-Tool-Node.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/18Pz3UyFN87OAQsYEmgVTrhW9PNv2IZ2M\n",
        "\"\"\"\n",
        "\n",
        "#pip install langchain_google_genai langchain langgraph langchain_core langchain_community\n",
        "\n",
        "# from google.colab import userdata\n",
        "# gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash', api_key=gemini_api_key, max_output_tokens=1024)\n",
        "\n",
        "from langgraph.graph import END, START, StateGraph, MessagesState\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, BaseMessage\n",
        "from typing import Sequence, Annotated\n",
        "\n",
        "class AgentState(MessagesState):\n",
        "    \"\"\"\n",
        "    AgentState extends MessagesState to maintain a message history.\n",
        "    The add_messages function ensures new messages are appended to the history.\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], \"add_messages\"]\n",
        "\n",
        "# Bind the functions for use in langgraph\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"it take two float numbers in arguments for adding.\n",
        "\n",
        "    Args:\n",
        "        a (float): The first number.\n",
        "        b (float): The second number.\n",
        "    Returns:\n",
        "        float: The sum of the two numbers.\n",
        "    \"\"\"\n",
        "    print(f\"add() called with parameters: a={a}, b={b}\")\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def subtract(a: float, b: float) -> float:\n",
        "    \"\"\"It takes two float numbers in arguments for subtracting.\n",
        "    Args:\n",
        "        a (float): The first number.\n",
        "        b (float): The second number.\n",
        "    Returns:\n",
        "        float: The difference or subtraction of the two numbers.\n",
        "    \"\"\"\n",
        "    print(f\"subtract() called with parameters: a={a}, b={b}\")\n",
        "    return a - b\n",
        "\n",
        "@tool\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"It takes two float numbers in arguments for multiplying.\n",
        "    Args:\n",
        "        a (float): The first number.\n",
        "        b (float): The second number.\n",
        "    Returns:\n",
        "        float: The product of the two numbers.\n",
        "    \"\"\"\n",
        "    print(f\"multiply() called with parameters: a={a}, b={b}\")\n",
        "    return a * b\n",
        "# Define a helper function to get the current time in Pakistan\n",
        "@tool\n",
        "def get_current_time_in_pakistan() -> str:\n",
        "    \"\"\"Fetches the current date and time in Pakistan Standard Time (PST).\n",
        "\n",
        "    Returns:\n",
        "        str: The current date and time in Pakistan formatted as 'YYYY-MM-DD HH:MM:SS'.\n",
        "    \"\"\"\n",
        "    pakistan_tz = pytz.timezone('Asia/Karachi')\n",
        "    current_time = datetime.now(pakistan_tz)\n",
        "    formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"get_current_time_in_pakistan() called, returning: {formatted_time}\")\n",
        "    return formatted_time\n",
        "\n",
        "\n",
        "\n",
        "tools = [add, multiply, subtract,get_current_time_in_pakistan]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# System message for the LLM\n",
        "sys_msg = SystemMessage(content='sys_msg = SystemMessage(content=\"You are a helpful assistant  that can answer general queries and perform general calculations. Use the tools available for general operations.\")')\n",
        "\n",
        "def chatbot(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Handles user interaction by appending new messages to the state\n",
        "    and invoking the LLM with the updated state.\n",
        "    \"\"\"\n",
        "    response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "    state['messages'].append(response)\n",
        "    return state\n",
        "\n",
        "# Build the StateGraph\n",
        "builder: StateGraph = StateGraph(state_schema=AgentState)\n",
        "\n",
        "# Add nodes and tools\n",
        "builder.add_node(\"ChatBot-LLM\", llm_with_tools)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define logic and connections\n",
        "from langgraph.prebuilt import tools_condition\n",
        "builder.add_edge(START, \"ChatBot-LLM\")\n",
        "builder.add_conditional_edges(\"ChatBot-LLM\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"ChatBot-LLM\")\n",
        "builder.add_edge(\"ChatBot-LLM\", END)\n",
        "\n",
        "# Compile the graph\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "# Visualize the graph\n",
        "from IPython.display import display, Image\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "x_qDv4e1smdk",
        "outputId": "15126fd1-0508-4cec-da2d-576b213d420f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6f4wwdsawRT",
        "outputId": "4bc64998-8e73-4bed-e85e-321cf2a62466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model='models/gemini-1.5-pro' google_api_key=SecretStr('**********') max_output_tokens=1024 client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e88d145faf0> default_metadata=()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Tools registered: {llm_with_tools.tools}\")\n",
        "print(f\"Available tools: {tools}\")  # List of tool functions\n",
        "# llm_with_tools = llm.bind_tools(tools)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSY0m0i-vZ3o",
        "outputId": "54c121a2-5582-41eb-8265-6850fc4d6ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available tools: [StructuredTool(name='add', description='it take two float numbers in arguments for adding.\\n\\n    Args: \\n        a (float): The first number.\\n        b (float): The second number.\\n    Returns:\\n        float: The sum of the two numbers.', args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x7e88cf396a70>), StructuredTool(name='multiply', description='It takes two float numbers in arguments for multiplying.\\n    Args:\\n        a (float): The first number.\\n        b (float): The second number.\\n    Returns:\\n        float: The product of the two numbers.', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x7e88cf396560>), StructuredTool(name='subtract', description='It takes two float numbers in arguments for subtracting.\\n    Args:\\n        a (float): The first number.\\n        b (float): The second number.\\n    Returns:\\n        float: The difference or subtraction of the two numbers.', args_schema=<class 'langchain_core.utils.pydantic.subtract'>, func=<function subtract at 0x7e88cf3968c0>), StructuredTool(name='get_current_time_in_pakistan', description=\"Fetches the current date and time in Pakistan Standard Time (PST).\\n\\n    Returns:\\n        str: The current date and time in Pakistan formatted as 'YYYY-MM-DD HH:MM:SS'.\", args_schema=<class 'langchain_core.utils.pydantic.get_current_time_in_pakistan'>, func=<function get_current_time_in_pakistan at 0x7e88cf396170>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "# def invoke_and_process(llm_with_tools, input_message):\n",
        "#     # Invoke the tool\n",
        "#     response = llm_with_tools.invoke([input_message])\n",
        "\n",
        "#     # Check if there's a tool call in the response\n",
        "#     tool_calls = response.additional_kwargs.get(\"function_call\", None)\n",
        "#     if tool_calls:\n",
        "#         # Parse the arguments and call the corresponding tool\n",
        "#         tool_name = tool_calls[\"name\"]\n",
        "#         tool_args = tool_calls[\"arguments\"]\n",
        "#         tool_args = eval(tool_args)  # Convert arguments from JSON string to dictionary\n",
        "\n",
        "#         # Find and call the tool\n",
        "#         tool_result = None\n",
        "#         for tool in tools:\n",
        "#             if tool.name == tool_name:\n",
        "#                 tool_result = tool.func(**tool_args)\n",
        "#                 break\n",
        "\n",
        "#         # Inject the tool result into the AIMessage content\n",
        "#         response.content = f\"The result of {tool_name} is {tool_result}.\"\n",
        "\n",
        "#     return response\n",
        "\n",
        "# # Example usage\n",
        "# human_message = HumanMessage(content=\"What is 2 + 2?\")\n",
        "# response = invoke_and_process(llm_with_tools, human_message)\n",
        "# print(response.content)\n"
      ],
      "metadata": {
        "id": "cejhfvZIMm5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "llm_with_tools.invoke([HumanMessage(content=\"What is 2 + 2?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScyRMeAcKzQo",
        "outputId": "e92c485c-6410-4962-cd3a-a75d0fd5c9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"a\": 2.0, \"b\": 2.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-ef2809fa-fbb6-4fbb-bb21-12ab9f7c97c7-0', tool_calls=[{'name': 'add', 'args': {'a': 2.0, 'b': 2.0}, 'id': '273f2a10-ed35-4785-9323-dea794eb262c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 353, 'output_tokens': 3, 'total_tokens': 356, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "# Initialize the chatbot state\n",
        "state = AgentState(messages=[])\n",
        "\n",
        "# Function to invoke tools and return a response through LLM\n",
        "def chatbot(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Handles user interaction by appending new messages to the state,\n",
        "    invoking tools if needed, and generating a natural language response through LLM.\n",
        "    \"\"\"\n",
        "    # Invoke the LLM with the current state\n",
        "    llm_response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "\n",
        "    # Check if a tool was called\n",
        "    tool_calls = llm_response.additional_kwargs.get(\"function_call\", None)\n",
        "    if tool_calls:\n",
        "        # Parse tool call details\n",
        "        tool_name = tool_calls[\"name\"]\n",
        "        tool_args = tool_calls[\"arguments\"]\n",
        "        tool_args = eval(tool_args)  # Convert arguments from JSON string to dictionary\n",
        "\n",
        "        # Execute the tool\n",
        "        tool_result = None\n",
        "        for tool in tools:\n",
        "            if tool.name == tool_name:\n",
        "                tool_result = tool.func(**tool_args)\n",
        "                break\n",
        "\n",
        "        # Create a message with the tool result and reinvoke the LLM\n",
        "        tool_result_message = HumanMessage(\n",
        "            content=f\"The tool '{tool_name}' calculated the result: {tool_result}. \"\n",
        "                    f\"Please craft a natural language response for the user.\"\n",
        "        )\n",
        "        final_response = llm_with_tools.invoke([sys_msg] + state['messages'] + [tool_result_message])\n",
        "\n",
        "        # Append the final response to the state\n",
        "        state['messages'].append(final_response)\n",
        "    else:\n",
        "        # If no tool was called, append the original response\n",
        "        state['messages'].append(llm_response)\n",
        "\n",
        "    return state\n",
        "\n",
        "# Function to simulate a chatbot session\n",
        "def chatbot_test():\n",
        "    print(\"Chatbot is ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Append user input as a HumanMessage to the state\n",
        "        state['messages'].append(HumanMessage(content=user_input))\n",
        "\n",
        "        # Get the chatbot response\n",
        "        updated_state = chatbot(state)\n",
        "\n",
        "        # Extract and print the latest AI response\n",
        "        ai_response = updated_state['messages'][-1]  # The chatbot's response is the last message\n",
        "        if isinstance(ai_response, AIMessage):\n",
        "            print(f\"Chatbot: {ai_response.content}\")\n",
        "        else:\n",
        "            print(\"Chatbot: Sorry, something went wrong.\")\n",
        "\n",
        "# Start the chatbot test\n",
        "chatbot_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "B2czDAc_M6Sf",
        "outputId": "7d9edf43-e823-4d9d-d763-805f610ebcf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot is ready. Type 'exit' to quit.\n",
            "You: what is time\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhausted",
          "evalue": "429 Resource has been exhausted (e.g. check quota).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-271-88e1e35c227a>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Start the chatbot test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mchatbot_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-271-88e1e35c227a>\u001b[0m in \u001b[0;36mchatbot_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Get the chatbot response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mupdated_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Extract and print the latest AI response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-271-88e1e35c227a>\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Invoke the LLM with the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mllm_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_with_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys_msg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'messages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Check if a tool was called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5350\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5351\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5352\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5353\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mtool_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_choice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         )\n\u001b[0;32m--> 949\u001b[0;31m         response: GenerateContentResponse = _chat_with_retry(\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             ) from e\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Do not retry for these errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFailedPrecondition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    831\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_since_first_attempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def parse_tool_request(user_input):\n",
        "#     \"\"\"\n",
        "#     Check if the user input is a tool request and extract tool name and arguments.\n",
        "#     Returns:\n",
        "#         tuple: (tool_name, arguments) or (None, None) if no tool is requested.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Example: User input format: \"add 5 3\"\n",
        "#         parts = user_input.strip().split()\n",
        "#         if len(parts) >= 3:\n",
        "#             tool_name = parts[0].lower()\n",
        "#             args = list(map(float, parts[1:]))\n",
        "#             if tool_name in ['add', 'subtract', 'multiply']:\n",
        "#                 return tool_name, args\n",
        "#         return None, None\n",
        "#     except ValueError:\n",
        "#         return None, None\n",
        "\n",
        "\n",
        "# def chatbot(state: AgentState) -> AgentState:\n",
        "#     \"\"\"\n",
        "#     Handles user interaction by appending new messages to the state,\n",
        "#     invoking tools if needed, and generating a natural language response through LLM.\n",
        "#     \"\"\"\n",
        "#     # Get the latest user message\n",
        "#     user_message = state['messages'][-1]\n",
        "\n",
        "#     # Check if the user message triggers a tool\n",
        "#     tool_name, tool_args = parse_tool_request(user_message.content)\n",
        "#     if tool_name:\n",
        "#         # Find and invoke the tool\n",
        "#         tool_result = None\n",
        "#         for tool in tools:\n",
        "#             if tool.name == tool_name:\n",
        "#                 tool_result = tool.func(*tool_args)\n",
        "#                 break\n",
        "\n",
        "#         # Create a response with the tool result\n",
        "#         tool_result_message = HumanMessage(\n",
        "#             content=f\"The tool '{tool_name}' calculated the result: {tool_result}. \"\n",
        "#                     f\"Please craft a natural language response for the user.\"\n",
        "#         )\n",
        "#         final_response = llm_with_tools.invoke([sys_msg] + state['messages'] + [tool_result_message])\n",
        "\n",
        "#         # Append the tool result and final response to the state\n",
        "#         state['messages'].append(AIMessage(content=f\"Result: {tool_result}\"))\n",
        "#         state['messages'].append(final_response)\n",
        "#     else:\n",
        "#         # Standard LLM invocation if no tool is needed\n",
        "#         llm_response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "#         state['messages'].append(llm_response)\n",
        "\n",
        "#     return state\n",
        "\n",
        "\n",
        "# # Function to simulate a chatbot session\n",
        "# def chatbot_test():\n",
        "#     print(\"Chatbot is ready. Type 'exit' to quit.\")\n",
        "#     while True:\n",
        "#         user_input = input(\"You: \")\n",
        "#         if user_input.lower() == 'exit':\n",
        "#             print(\"Chatbot: Goodbye!\")\n",
        "#             break\n",
        "\n",
        "#         # Append user input as a HumanMessage to the state\n",
        "#         state['messages'].append(HumanMessage(content=user_input))\n",
        "\n",
        "#         # Get the chatbot response\n",
        "#         updated_state = chatbot(state)\n",
        "\n",
        "#         # Extract and print the latest AI response\n",
        "#         ai_response = updated_state['messages'][-1]\n",
        "#         if isinstance(ai_response, AIMessage):\n",
        "#             print(f\"Chatbot: {ai_response.content}\")\n",
        "#         else:\n",
        "#             print(\"Chatbot: Sorry, something went wrong.\")\n",
        "\n",
        "\n",
        "# # Start the chatbot test\n",
        "# chatbot_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "xm8aJwMjUsk1",
        "outputId": "fcc3d475-7f39-429a-e3b7-ee7d16508d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot is ready. Type 'exit' to quit.\n",
            "You: what is 2+2?\n",
            "Chatbot: 2 + 2 = 4\n",
            "You: what is 34567+34567?\n",
            "Chatbot: 34567 + 34567 = 69134\n",
            "You: multiply 3 into 3?\n",
            "Chatbot: 3 multiplied by 3 is 9.\n",
            "You: what is the current date?\n",
            "Chatbot: I do not have access to real-time information, including the current date.  To get the current date, you should check a clock or calendar on your device.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-222-37e953adfad2>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Start the chatbot test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mchatbot_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-222-37e953adfad2>\u001b[0m in \u001b[0;36mchatbot_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot is ready. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wh# Test the tools using structured input\n",
        "print(add.invoke({\"a\": 2, \"b\": 2}))        # Should return 4\n",
        "print(subtract.invoke({\"a\": 5, \"b\": 3}))   # Should return 2\n",
        "print(multiply.invoke({\"a\": 3, \"b\": 4}))   # Should return 12\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ck5YkpzxDlp",
        "outputId": "9fdc75d9-21b3-41a0-aab7-97270c8267f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n",
            "2.0\n",
            "12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New\n"
      ],
      "metadata": {
        "id": "IoAKuC36VcGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBHgwTYZs_NB",
        "outputId": "d0918a91-01b7-420c-f099-f81cc122172f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.15.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.3.29)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_groq) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_groq) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_groq) (9.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_groq) (3.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_groq) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_groq) (2.3.0)\n",
            "Downloading langchain_groq-0.2.3-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.15.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
            "Successfully installed groq-0.15.0 langchain_groq-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahsan GEmini API Key\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('GROQ_API-KEY')\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama3-8b-8192\",api_key=groq_api_key)\n",
        "\n",
        "# llm = ChatGroq(\n",
        "#     model=\"mixtral-8x7b-32768\",\n",
        "#     temperature=0,\n",
        "#     max_tokens=None,\n",
        "#     timeout=None,\n",
        "#     max_retries=2,\n",
        "#     # other params...\n",
        "# )"
      ],
      "metadata": {
        "id": "wzRFWxtho_tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, START, StateGraph, MessagesState\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, BaseMessage\n",
        "from typing import Sequence, Annotated\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "class AgentState(MessagesState):\n",
        "    \"\"\"\n",
        "    AgentState extends MessagesState to maintain a message history.\n",
        "    The add_messages function ensures new messages are appended to the history.\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], \"add_messages\"]\n",
        "\n",
        "# Define helper functions for arithmetic operations\n",
        "from typing import Dict\n",
        "\n",
        "@tool\n",
        "def haircut_service(style: str = \"classic\") -> Dict[str, str]:\n",
        "    \"\"\"Provides a haircut service and returns the price and time required.\n",
        "\n",
        "    Args:\n",
        "        style (str): The desired haircut style (e.g., 'classic', 'fade', 'modern').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary containing the price and estimated time for the haircut service.\n",
        "    \"\"\"\n",
        "    price = \"1500 PKR\"\n",
        "    time = \"30 minutes\"\n",
        "    result = {\"price\": price, \"time\": time}\n",
        "    print(f\"haircut_service(style='{style}') called, returning: {result}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "@tool\n",
        "def beard_grooming(beard_style: str = \"trim\") -> Dict[str, str]:\n",
        "    \"\"\"Performs beard grooming and returns the price and time required.\n",
        "\n",
        "    Args:\n",
        "        beard_style (str): The desired grooming style (e.g., 'trim', 'shave', 'full styling').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary containing the price and estimated time for the beard grooming service.\n",
        "    \"\"\"\n",
        "    price = \"1000 PKR\"\n",
        "    time = \"20 minutes\"\n",
        "    result = {\"price\": price, \"time\": time}\n",
        "    print(f\"beard_grooming(beard_style='{beard_style}') called, returning: {result}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "@tool\n",
        "def face_cleaning_service() -> Dict[str, str]:\n",
        "    \"\"\"Provides a face cleaning service and returns the price and time required.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary containing the price and estimated time for the face cleaning service.\n",
        "    \"\"\"\n",
        "    price = \"1200 PKR\"\n",
        "    time = \"25 minutes\"\n",
        "    result = {\"price\": price, \"time\": time}\n",
        "    print(f\"face_cleaning_service() called, returning: {result}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "@tool\n",
        "def facial_service(type_of_facial: str = \"hydrating\") -> Dict[str, str]:\n",
        "    \"\"\"Performs a facial treatment and returns the price and time required.\n",
        "\n",
        "    Args:\n",
        "        type_of_facial (str): The type of facial to provide (e.g., 'hydrating', 'anti-aging', 'acne treatment').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary containing the price and estimated time for the facial service.\n",
        "    \"\"\"\n",
        "    price = \"2000 PKR\"\n",
        "    time = \"45 minutes\"\n",
        "    result = {\"price\": price, \"time\": time}\n",
        "    print(f\"facial_service(type_of_facial='{type_of_facial}') called, returning: {result}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "@tool\n",
        "def hair_spa_service() -> Dict[str, str]:\n",
        "    \"\"\"Provides a hair spa treatment and returns the price and time required.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary containing the price and estimated time for the hair spa service.\n",
        "    \"\"\"\n",
        "    price = \"2500 PKR\"\n",
        "    time = \"60 minutes\"\n",
        "    result = {\"price\": price, \"time\": time}\n",
        "    print(f\"hair_spa_service() called, returning: {result}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "@tool\n",
        "def manicure_pedicure_service(option: str = \"both\") -> Dict[str, str]:\n",
        "    \"\"\"Provides manicure and/or pedicure services and returns the price and time required.\n",
        "\n",
        "    Args:\n",
        "        option (str): The desired service option ('manicure', 'pedicure', or 'both').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary containing the price and estimated time for the selected service(s).\n",
        "    \"\"\"\n",
        "    if option == \"manicure\":\n",
        "        price = \"1500 PKR\"\n",
        "        time = \"30 minutes\"\n",
        "    elif option == \"pedicure\":\n",
        "        price = \"1800 PKR\"\n",
        "        time = \"35 minutes\"\n",
        "    else:  # both\n",
        "        price = \"3000 PKR\"\n",
        "        time = \"60 minutes\"\n",
        "    result = {\"price\": price, \"time\": time}\n",
        "    print(f\"manicure_pedicure_service(option='{option}') called, returning: {result}\")\n",
        "    return result\n",
        "\n",
        "# Define a helper function to get the current time in Pakistan\n",
        "@tool\n",
        "def get_current_time_in_pakistan() -> str:\n",
        "    \"\"\"Fetches the current date and time in Pakistan Standard Time (PST).\n",
        "\n",
        "    Returns:\n",
        "        str: The current date and time in Pakistan formatted as 'YYYY-MM-DD HH:MM:SS'.\n",
        "    \"\"\"\n",
        "    pakistan_tz = pytz.timezone('Asia/Karachi')\n",
        "    current_time = datetime.now(pakistan_tz)\n",
        "    formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"get_current_time_in_pakistan() called, returning: {formatted_time}\")\n",
        "    return formatted_time\n",
        "\n",
        "# Bind the functions for use in langgraph\n",
        "\n",
        "\n",
        "tools = [haircut_service, beard_grooming, face_cleaning_service, facial_service, hair_spa_service, manicure_pedicure_service, get_current_time_in_pakistan]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# System message for the LLM\n",
        "sys_msg = SystemMessage(content=\"\"\"\n",
        "You are a highly knowledgeable and helpful assistant specializing in men's grooming services.\n",
        "Your primary role is to provide accurate, detailed, and user-friendly information about a range of men's grooming services, including but not limited to haircuts, beard grooming, face cleaning, facials, hair spa treatments, and manicure/pedicure services.\n",
        "\n",
        "You can offer:\n",
        "\n",
        "Service Descriptions: Provide clear and concise explanations of each grooming service, detailing the processes and their benefits.\n",
        "Price Ranges: Offer accurate price estimates based on the type of service, customization, and location-specific rates if applicable.\n",
        "Time Estimates: Give realistic time durations for each service to help users plan their schedules.\n",
        "Recommendations: Suggest services based on user needs, preferences, or occasions (e.g., special events, regular upkeep).\n",
        "Guidelines for Interaction:\n",
        "Clarity and Detail: Always explain the requested information clearly and thoroughly, avoiding jargon unless necessary.\n",
        "Customization: Tailor your responses to the user's specific needs, such as hair type, skin condition, or preferred style.\n",
        "Engagement: Maintain a polite, professional, and approachable tone, ensuring user comfort and satisfaction.\n",
        "Available Functions for Responses:\n",
        "haircut_service\n",
        "\n",
        "Provide information about haircut styles, trends, and maintenance.\n",
        "Offer price ranges and time estimates for various haircut options (e.g., trims, fades, layer cuts).\n",
        "beard_grooming\n",
        "\n",
        "Explain beard grooming services such as trimming, shaping, and styling.\n",
        "Include advice on beard care products and maintenance routines.\n",
        "face_cleaning_service\n",
        "\n",
        "Describe face cleaning procedures, including exfoliation, cleansing, and moisturizing.\n",
        "Highlight benefits like improved skin texture and reduced acne or blackheads.\n",
        "facial_service\n",
        "\n",
        "Outline different types of facials (e.g., anti-aging, hydration, brightening).\n",
        "Provide insights into the steps involved and the skin concerns they address.\n",
        "hair_spa_service\n",
        "\n",
        "Detail the hair spa process, such as deep conditioning, scalp treatments, and massages.\n",
        "Emphasize benefits like strengthened hair, reduced hair fall, and a healthy scalp.\n",
        "manicure_pedicure_service\n",
        "\n",
        "Explain the importance of hand and foot care, including nail trimming, cuticle care, and massages.\n",
        "Highlight benefits like improved hygiene and relaxation.\n",
        "Response Requirements:\n",
        "Accurate Details: Always verify prices and durations when possible.\n",
        "Relevance: Avoid unnecessary information; focus solely on the requested service.\n",
        "User-Centric Approach: Ensure responses are actionable and easy to understand.\n",
        "Tools and Operations: When a tool or specific operation is required, clearly outline its role in addressing the query.\n",
        "Example Interaction:\n",
        "\n",
        "User: \"How much does a haircut cost, and how long will it take?\"\n",
        "Assistant:\n",
        "\"Haircut services generally range between $100 and $150, depending on the style and salon. A basic haircut typically takes 30 minutes. For trendy or complex styles, it may take up to 1 hour. If you need details about a specific type of haircut, let me know!\"\n",
        "\n",
        "User: \"What are the benefits of a facial?\"\n",
        "Assistant:\n",
        "\"A facial helps cleanse and exfoliate the skin, remove dead skin cells, and treat specific concerns like acne, dryness, or uneven tone. It also promotes relaxation and improves blood circulation, giving your face a fresh and rejuvenated appearance.\"\n",
        "\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "def chatbot(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Handles user interaction by appending new messages to the state\n",
        "    and invoking the LLM with the updated state.\n",
        "    \"\"\"\n",
        "    response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "    state['messages'].append(response)\n",
        "    return state\n",
        "\n",
        "# Build the StateGraph\n",
        "builder: StateGraph = StateGraph(state_schema=AgentState)\n",
        "\n",
        "# Add nodes and tools\n",
        "builder.add_node(\"ChatBot-LLM\", llm_with_tools)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define logic and connections\n",
        "from langgraph.prebuilt import tools_condition\n",
        "builder.add_edge(START, \"ChatBot-LLM\")\n",
        "builder.add_conditional_edges(\"ChatBot-LLM\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"ChatBot-LLM\")\n",
        "builder.add_edge(\"ChatBot-LLM\", END)\n",
        "\n",
        "# Compile the graph\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "# Visualize the graph\n",
        "from IPython.display import display, Image\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "yRcizQXBVb-U",
        "outputId": "25435eaa-0ba4-433f-ffb5-4f918bf15efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANYAAAD5CAIAAADUe1yaAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXlcVFX/x88szL4Bw7CKIIsCAmKIgKi5FwLupIa2GGpm5mOW5lJWTz5pZuWCaeRSltujRi5h7igk5IYiiDCA7MsMzL4vvz/GH/HosA33zpkZ7/vly9fcO3PP+TB8OPv5HpzRaAQYGPDAwxaA8byDWRADMpgFMSCDWRADMpgFMSCDWRADMkTYAqxNQ5VSIdErpHq9zqhRGWDL6REkCp5MxdNZRDqb4OpJhi0HYXDPw7ig0WAsKZBWFMmqHih8B9GITjgak8DhkTRK+7AgDg/EAq1coqPQCY2VKr8wekA43SeYBlsXMji+BW9fart7VdQ/hDZgMMN/MB22nL4iadVWPZA316pFTdq4ZFfvACpsRX3FkS1YU6o491NjyHDWiBQubC3I01Cl/OuU0NmdNCaVB1tLn3BYC9650lZbphw/151KJ8DWgiI1ZYo/9jbO+bAf09kJthYLcUwL3s8Vi1u0CVMdsPB7FrVSf2hzzeyV/Sj2+cfmgBbMOdkCDGDUDDfYQqzKgc+rUhZ6ObuTYAvpNY42LlicL9GqDM+b/wAAaWv6H9pcDVuFJTiUBZtrVHXlinFz3GELgQCBgEt93+fcz42whfQah7LgtZOCsDg2bBXQ4HpRcACU3pLCFtI7HMeClQ/kZCrea4Ddj5P1hfhkbt4pAWwVvcNxLFh6UzpiynPRBe4CBoc4OJ5dnC+GLaQXOIgFRS2allq1M89K/UGZTPbw4UOLH29oaKivr0dU0T94+lNKb8pQShwNHMSClUVya06+zZ49Oysry7Jna2trU1JSiouLkRb1BJ8gWlO1Squ2j+lvx7Fgc406INJ6FtRoNJY9aDQadTod2mOxobGsxyVyVLNAEAexYF25kuWCygzV/v37ExMTExISFixYUFBQAABISkpqbW09duxYdHR0UlKSyZE7d+5MSUkZPnz45MmTMzIy9Hq96fFNmzZNnDgxJydn2rRp0dHRf/zxx8yZMwEAq1evjo6O3rBhAxqayRR8a5MWjZTRwEHWCyqkejoL+Z+loKBgx44dL730Unx8fF5enkKhAABs3rx56dKlL7zwwquvvkoikQAABAIhPz9/1KhRPj4+paWle/fuZbFYaWlppkRkMllGRsbq1auVSmVcXBwej1+3bt3ixYujo6NdXFwQ1wwAoLOILfVqNFJGA0ewoFyiozFRmR41dRpSU1MjIiISExNNN0NDQ4lEIpfLHTJkiOkOgUA4cOAADoczXdbW1l66dKndghqNZt26dYMHDzZdDho0CADg5+fX/jji0NnEKvupiB3Bgga9kcpAxYIJCQksFmv9+vUffPBBQkJCF59sbW394Ycfbty4IZFIAABMJrP9LQqF0u4/60AgAgIBZ80c+4IjtAXpLGJrk4X9g67hcrl79+7t37//8uXLFyxY0NzcbPZjQqHw1VdfLSgoePvtt7dv3x4SEtLeFgQA0GjWXt4sE+lJVLv5zdqN0C7AE3BkKl4p0/fgs73Gz89v27Ztu3btKi8v79h76NirPX78eGtra0ZGxqRJk8LCwjw8PNBQ0nPkEh0aLWOUcAQLAgB8B9IUUh0aKZvGX4YNGzZy5Mj24WgqlSoQ/DMPJhKJnJ2d250nEom6GHahUCgAgJaWFjTUmtDrjRye3axgtZu/la5hc5349+SI7y578ODBqlWrUlNTaTRaXl5eaGio6X5UVFR2dvb+/ftZLFZERER0dPTRo0d37doVGRl56dKl3Nxcg8EgEok4HM6zabq7u3t7ex88eJBKpYrF4tmzZ5PJCMsu/kvyysp+yKaJHg5SCvoPplcWId8HJJFI/v7++/bt27FjR1RU1Pr16033ly1bFh0dnZmZuW/fvpqamrFjx7711lvHjh1bu3atVqvdv3+/n5/fkSNHzKaJw+E2btxIp9O3bNly6tSp1tZWZDU3V6voHKIdVcSOs2r699314+fyaEy7+epR4u6VNoDDDRltpgC2TRznFxYQSb9xpnXs7E63k61atSo/P//Z++7u7k1NTc/eZ7PZFk8E95zr16+vW7fu2ftGo9FoNOLxZqqpM2fO0OnmZyMNBmPu78J3tgaioBQtHKcUBAD8/MXj5IWeHDfz62WEQqFabWbOQKvVOjmZabzj8Xgr9G1VKpXZuthgMBgMBiLRTBnh4eFh1poAgOtZAjqLEDXGGQWlaOFQFqwsktWWKUdOe+42jphQyvXnDzamLPKGLaR3OEh3xIT/YAbRCX/zAsINfHvhyJYae9zW7lAWBADEJbk2VKiKb9jTsmFEOLmzdvRMN3vc0O5QFXE7l4828/qRn5+tTCcz6hKmcN287TLolqOVgibGpPIaqlS5v9vZRh4LkIt1+zZUDh3DsVP/OWwpaKLwqujWxbb4ZNdBw1iwtSCPRmXIOy2QCHVjX+ExOHY8uObIFjRN2OedEkqE2oBIhv9gOtvV/ppKz1JbpmioVN2+1BafxA1PsPvGhoNb0ISwQV18Q1JZJCeS8D5BVDIVT2cTmc5Oer19/OxGPZC2aWViHQ4HinLFPF9K4BB6+Ai7mf/omufCgu0IG9RN1SqZSC8X6wgEnFSE8OKa8vJyNzc3NhvhkonGJBBJOAabyHRx8h1EI5EdqgX/fFkQbZYvXz5jxoyRI0fCFmJPONTfE4Y9glkQAzKYBZHE3d3d7MICjC7ALIgkTU1NOh0q+wccGMyCSEKlUtt3E2P0EMyCSKJUKrERht6CWRBJ2Gx2Z4tJMToD+76QRCwWGwx2E1XNRsAsiCSenp5m9wBgdAFmQSRpaGjQau0mqpqNgFkQAzKYBZGEwWBg3ZHegn1fSCKTybDuSG/BLIgkTCaTQLDLswghglkQSaRSacfIghg9AbMgBmQwCyKJm5sbVhH3FsyCSNLS0oJVxL0FsyAGZDALIgm2ZNUCMAsiCbZk1QIwC2JABrMgknh5eWEVcW/BLIgk9fX1WEXcWzALYkAGsyCSYD1iC8AsiCRYj9gCMAtiQAazIJJg+4gtALMgkmD7iC0AsyCSYCtlLACzIJJgK2UsALMgBmQwCyIJi8XCdtD1Fuz7QhKJRILtoOstmAWRxNPTE5sd6S2YBZGkoaEBmx3pLZgFkQRbrGUBmAWRBFusZQGYBZHE2dkZKwV7C3b0DQJMnDiRTCbjcDiRSESlUkkkEg6Hc3JyOn78OGxpdgD2J4sAzs7OfD7f9FqhUAAADAbD/PnzYeuyD7CKGAGmT59OJv/PccA+Pj5z586Fp8iewCyIANOmTfPx8Wm/NBqNo0eP5vF4UEXZDZgFEYBEIk2bNq29IPT29k5LS4Mtym7ALIgM7QWhqQh0d3eHrchuwCyIDGQyOSkpiUgk9uvXDysCe4V99Iilbdq2Jo2ND/rGDE6+7Fc8dOhQeQujokUOW05X0OgEFy8nEtkmVtfa+rigoF6dd0oobND4htDlSB+i/tyiVRtam1SBQ5hjZsHvM9m0BUUt2lN76sfP82KwsfNkkKekQNRUpUxO94Qrw3YtqFbqD3z2eM7qAbCFODLldyWNlYqXX/eAqMF2uyP52a3xKfCrCccmcAjLaAD1FUqIGmzXgnXlSqYLVv+ijhMZL2zQQBRguxYEADCdMQuiDseDLBfD7OfZrgWlbTqDjTZTHQq9xqjTwvyibdeCGM8JmAUxIINZEAMymAUxIINZEAMymAUxIINZEAMymAUxIINZEAMymAUxIINZEAMyjmbBkocPVn20LHnKiy9PTpj32vR9+7833V/38fuLFvd6S0djY0NDY337pVgsGjMu2vRvyrRxKz9YUlJSZEE6z3Ll6oUx46Lv37/bw/umPfMzZk3qGM6wtrbagp8ROg5lwRs3ri97b8GjRyUTJkyePm12YEBwU1OjxanV1dfOTUspLS1+6v6okWM/+fjL9LeWtrYJP1j1TrdZdJZOH6msLG9tFT54cK/9zo3865VVfLuLq2Qf25d6glgi/s+mT3g8j53b93E4zn1PUK/TmV1SHhAQ/OLo8QCA4OCQRYvTbt66MTlxqgXp9BF+RRkA4Nr1y+HhQ0x3bty4rtVqq6oqAgODEc8OPRynFMzO/l0iES94c0kX/tt/YM+MWZOmTh//7XdfajRP1mn+kf37osVpEybFpkwd++8v1opEbQCAhsb6196YCQD49LPVY8ZFf7l5w7OpUciUjpdCoeDfX6w1tQE+XLW0oqK8h+lYRmVlOQAgN/eK6VKhUBTeuw0AKCt/iFQW1sFxSsFbt/LJZPLoUeM6+8CjsodkCmVR+rKy8tL/Hv/VxYU7f95bAIDi4vu+vn4TJiS2tbWeOHlYrpD/54tvXV24a9f8+4uN6954fXHUkGhnZ5f2dAwGvV6vFwha9mRu79/ff+yYSQAAlUq1YuViiUS8MH0ZhUw5dOTAipWLf/7pZBfp9BF+RZm3d7+6uho+vywgIOj2nQKdTuft5VNW9vDll1KQysUKOI4Fm5ob3d09uzh5xsvL55uvdxMIhIkTJ1dXV165et5kwRX/WtN+aheRSDz4y161Wk0mk4ODBgEAfH392ms6Ewd++uHATz8AAJhM1vp1G6lUKgDg/IWz1dVVX2/ZNTRqGAAgPDxqblrKiROHX5uf3lk6faSyonzGjDkXLvxxPfdKQEDQjRvXQ0IGBwUOLCsvRTAXK+A4FjQajSQSqYsPMOiMdoP6+QUUl9w3vdZqtSdOHj5/4WxzcyOZTDEYDCJRm7t7p5vKpk6ZlZg4VSwW3bx5Y9Xqdxcvei91Vlph4S0GnWHyHwDAw8PT19ev9JH5LoharW5tE5pe89zcLTiwqampUSaX+fkFjB49/vr1y6/NT88vyJ0xfQ6ZTDl/4azRaLSjo/Acx4IuLq5lZT1tBhEIBFPP0Wg0rlm7vPRR8WvzF4aGRly7dunwkZ8Mxq4ObnB2dg0KHAgAiH5huFDYsnffrpTkmTK5jP2/bVAWiy0UtJhNobjk/or3F5te//dotqsrt8c/5RNMDcEB/oFeXj6/Htp//foVgaBl5MixgpZmpVIpErUhWOOjjeNYcHBYZGHh7by8nPj4UT1/qrDw9q3bBWvX/Hv8uJcAAHW11b3K1NvbV61WNzc3unF5xcX3O77V2ip055kvSgf4B37+2RbTayaT1ascTfArypycnLy8fIhEopen946MLQEBQd5ePqbUWgTNdmRBx+kRv/zyFCKRuPuHbWKxqP3mlasXun5KLBEBAEzNtfZL03gvmUwBAHRWkpkoKrqLw+HYHOewsAipVNI+Us3nl9XV1Zgaf8+mw2ZzEka8aPrXdeOhMyory/v162+Kaz169PimpsZRI8cBAFhMFpfr1tLcZEGasHCcUtDby2fxwvd2ZHz9xoLUMWMmUinUW7cLHj584PptZhf9gNCQcBKJ9EPmjsmTp1VUlP16aJ+ppe/t5cPjuXt5eh/970EKlSqRiKdPm216hM9/dDXnolQqyfsr59btgpTkGWwWe/y4l3/5dd+Gz1bNS3sLj8f//HMmh+M8JWUWAODZdJ4KydrOuT9P37l7s/0yNjahs/v8irLAgCeDf6NHjz90+MDIhDGmywH+gS2CZiS+USvhOBYEAMyYMcfdw/PwkZ/Onv0NABAUNOjzT7d03Q91c+OtW/vFzoyvN3z6YVhoxNavd+/b//2Jk4cTEl7E4XDr1m3c/NWnO3Zu4fE8xrw40dT5zbl2KefaJQqF0s+n/7+Wf2QalyYSiV9t2pmxa+uu778xGAwR4VHvLHnfVBs+m46Hh/kwLmfO/tbxks3msNmcZ+8zGIza2urx4142XQ4MDokZFufvH2C69PcPFNiVBW03psyeNRXT3/MjUxynqWCbPMgT6TS6hCm97hIhBfYLxoAMZkEMyGAWxIAMZkEMyGAWxIAMZkEMyGAWxIAMZkEMyGAWxIAMZkEMyGAWxIAMZkEMyGAWxICM7S7WcvMhAyzkPvoQiDi45yHabimIwwFhgxq2CsensUrBcoV5vovtWjAgnN5Sp4KtwvFRyvS+wVSIAmzXguEJHFGTuqRA1IPPYljI+YN1L4zjkKgwK2LbXTVt4rdddVwvKtuN5OZNBvazN9bGUcl0wkb1/ettY1/h+Q6kwRVj6xYEABTni6uKFQY9ENT1umloNBpVKhWVQkHJvlqtloDH4zvZi67RaIhEIh6PSlWjVCoNBgMOh8Pj8ab/2190+yyD4+TmQ4p6kQO3FWjCDixoMWfOnNm0adPXX389bNgwlLJITEzct2+fu7u72XczMjLIZPKCBQvQyDovL2/Dhg2tra2m36DRaGQwGHQ6nUgknjp1Co0cUcJ2B2X6yAcffEClUnNyctDLQq1Wr1mzpjP/AQBSU1Pr6upQyj0+Pj4iIuLy5cum2B04HE6hUCgUCrsrUxywFLx27dqKFSs2bdo0duxY2FrQpaioaNWqVU1N/2xcx+PxBQUFUEX1GtvtEVvGZ599dvLkyfz8fCv47+LFixcudBOtYevWrTKZDCUBgwcPjo2N7Xhn48aNKOWFHo5jwaKionHjxkVGRm7duhWlHsBTnD59uttwHFVVVYWFhehpSE9P9/HxMb3m8Xjnz59fu3YtetmhgYNUxBkZGS0tLe+99x6Hw7FapjKZjE6ndx1GTaVSAQAoFEoXn+kj27ZtO3jwoJOTU25uLgAgOzv7+PHj6enpMTEx6GWKJEY7RyQSpaWlZWZmwhYCk6SkpI6XSqVy8eLFe/bsgaeoF9i3BS9cuPDuu+8+ePDA+llfvnw5IyOj248plcp58+ZZRdHTZGVlpaSkPH78GEruPceO24KbNm06d+7ctm3bQkNDrZ/7xYsX+/fv3+3HKBSKXq9/+BBCCPKUlJSdO3e+9957WVlZ1s+959hlW1CtVq9bt27YsGGpqamwtXSPTqfD4XAWRPNFit27d5eXl3/11VewBHQD7GK419y9ezcuLq6iogKiBr1eL5PJevhhg8Gg1+tRVtQNFy9eHDNmDNwvrTPszIJHjhx54403YKsw7ty5s1cdoGHDhul0OjQVdY9IJFq6dOnp06fhyngWe2oLbt68+fHjx3v37oUtBBQUFEyaNKnnn58wYYJpxAQibDZ7+/bt+fn5mzdvhqvkKeymLbhixYrhw4e/8sorsIXYPUeOHCksLLSheRTYxXCPmDt37pUrV2CreAKfz6+tre3VIwaDoaSkBDVFveb27dspKSmwVTzBDiz44Ycf3rx5E7aKf4iLi1OpVL196qOPPsrOzkZHkSXU1NTExMSIxWLYQmzegomJib0tclDl/v37ljnp4cOHu3fvRkGR5Wi12rS0tLa2NrgybLotOHny5B9//NHDo9OzuDD6zrhx444dO+biAu2oHNvtEa9cuXLPnj025b/a2tqzZ89a/PijR4/u3jVzvjpcLl68mJqaqtfroSmAWwh3xrvvvnv9+nXYKp5m3rx5RUVFfUkhJiZGq9UipwgxII5c2mJFvHXrVnd391dffRW2kP9BJpOJxWJvb+++JPLo0SM8Hh8YGIicLmRoaGhIT08/ffq09bO2OQueOXOmurr67bffhi3kaaRSKY1GgzjVizZ37949evSo9ccLbastKBAItm3bZoP+O3r0aEZGBiL+y8rK2r9/PxKiEGbIkCGenp4QtEGp/jvjzTffvHPnDmwVT6PX61euXIlggnPnzm1ubkYwQQSZNm1aVVWVNXO0oYr48OHD9fX1K1asgC3kuebx48dbt2797rvvrJajDVXEW7ZssUH/lZeXnzhxAvFkb968WVtbi3iyfad///5sNvvMmTPWy9KaRW4X7N27d/v27bBVmOGFF15AKeX4+HilUolS4n2hpqbGmjPItlIRjxw58ty5czQa5BA7T2EK2tL1HjmLEYvFDQ0NgwYNQiPxPrJ169bw8PAJEyZYIS+bqIjPnDkzZ84cW/NfZWVlcXExSv4zLeDr16+faZenrTF8+HCrBaaxCQv++eefERERsFX8D0VFRRs2bBg8eDCqudDp9HfeeccGZ+1GjBhx8+ZNtdoaUW7hW9BgMOTl5SUkJMAW8jQHDhywQi4//vjj33//bSPNoY7MnDnz2rVrVsgIvgVzc3NnzZoFW8X/UFtba805tPT0dPSqe4vx9PS0TvEM34L37993dXWFreIf1q9ff+/ePVRDcDxLTk7Oxx9/bM0cuyU4OLisrMwKGcGPL8jn85OTk2GreEJ5efn8+fODgoKsnO+oUaNoNFpOTs6oUaOsnHVnDBw4kEi0hj3gW7CqqiogIAC2CmAKncvlcq0ZGKkj0dHRUPLtDAaD8ffff+v1erRXZsCviBsaGng8HmwV4NatW1YOzGWWTz75JDs7G66GdphMplQqRTsXyBbUaDRGo5FMJsOVIZfLxWLxnj174MoAAHz66acajaayshK2EAAAiIqKQi8+ZzuQK2KZTMZgMOBqMBgMQqHQdqICp6SkwJbwhJKSEis0ByGXglqt1tPTE6IAvV4fGxvr6+sLUYNZFixYAH3IWqvVOjmhfioEZAuSyWT0YtL3hLy8vL/++guigM748ccfNRqNSATz8CmJRMJisdDOBbIFKRQKxElSPp8fFxdns2vxY2JiOBwOrLJQJBLR6XTHLwUpFAqLxdLpdNbPetKkSWw22zpDX33h+PHjfD6//XLKlCmff/65FfJtaWmxTrRq+IMyTk5OHU/OsA73798/deoUl8u1cr4W8PnnnxcXFyuVStNlbW1tYWGhXC5HO9/y8nLr1A/wLTho0CChUGjNHI8cORIeHt7teQ22Q3JyslqtPnHixNChQ3E4XGNjY15eHtqZCgQC6yxfgm9BOp1eVVVltexSU1OTkpKslh1ScDicjRs3ms5TUSgU586dQzvHv//+u4+bpnsIfAsOGDCgoqLCatnt2rWLTqdbLTukGD58ePtrPB7P5/PRbr1IpdKBAweimoUJ+BYMDg62TkWcmZkJALCpVTk9JCYm5qkeW2NjI6qL+ZqbmxsbG63TVoZvwaCgoBs3bqCdy8SJE19//XW0c0GJ9PT0kJAQNpvdvrJVq9WeP38evRxLSkpCQkLQS78j8IckXFxcfH19m5ubeTzezJkzDQYDspsmTXOA2dnZ1jmYDg3S09PT09Nra2vz8vKuXLnC5/M1CnxzvfT+nXI/Pz80ciy+VxkSNFTa1qfBMicSjkLvvk8NeQddSkqKXq9vaWkxHYuAx+NjY2N37NjRlzSnTp2q0+lMEXpEItHatWt37tyJnGSYaDWGaycFZXekbA9jW6OWSqWilJFer8PjCX1cy01jEeRifWgsc/hLXTV+YJaCKSkp9fX1ptemvZJ4PL6Pm0gOHTrU1NSk1WqnTJmSlZWVmZnpMP5TyfX7NlSNS/OMfNGVRLHRGZ2nkIu1VcWy3/fUJ6d7dmZomHXTsmXL3NzcOt7hcrnh4eF9SfPPP//UaDQAgLq6uhkzZqxcubLPMm2FzHWVaesCPP1o9uI/AACd7RQW5+wTTD+1p6Gzz8C04Pjx45OTk9uHSIxGI5VKDQsLszjBhw8fCgSC9r+2x48f28UJYT3h2m+CMbNtKOBsrwiKYrNcSWV3za9+hdxCX7JkSXR0tKk9isPhIiMj+5LapUuXGhsbO96pqKiYPn16n2XC53GJnOVqN9M5z0KhE5qqzO9Kht9J/OKLL4KDg02bFfo4L56Tk2MwGEyvTS94PJ49DkQ/hdFoJNMIHDc7tqCLJ1mtMph9C/6gDIVCWbNmzfr16wEAfamFb968KZFITKUpj8fj8XixsbFxcXG2FqfBAnA4XFOVLcb96DkGPZB1MsTTVwvW8xVigU4u1SkkeoMe6HTmnd4dvMkxq6urq0uvk0qBhfNO9+61hfFmxwYwPTw83NzcmEwmAEBeQ/yrRkhnEWhsoncAldqDYSoMK2OhBR+XyB/dllUUyZ09qEYjjuBEwDsR8ASCxWOMXPcQrnuIVGHp8wD4B8b4d4iAYEpKpsTpNVq9VkPAay7+2szhkYKj6BEjOQSizUUveG7ptQUbKpU5J4VONBKOSA6IcyY62U254jrAVSFS8YsVf53mvzDBJWaSsw2G0XgO6Z0FLxxqqa9Qufq70J2tGu8CKWgcCo1D4Q5wqeG3FX3yeGKae79gtCYYMHpIT3vEOq1h/2ePVXqy71AvO/VfR7gDnP1jvK8cF9650gZby/NOjyyo1xn3fFThGerOcLX7AY528AR8vyGe5fc1D25IYGt5runeggaDcdeH/NBx/mQ66puprI9bILcoX3HjrFV3DmB0pHsL/vKf6qB4ayzghoV7sFtliZp/D/XIFRhm6caCV44LOP04ZLodj8v3BM9Q99tXJJJWDWwhzyNdWVBYr64skjPdIMd8sQ4kJv3qCaw6hkBXFsz5Tcj1h3ZSspVhezCE9dqWWmsE+MboSKcWbKxS6vR4ppttHcRg4pdjH2/6DvlVWNwBLneuihFPFhYymexR2cM+JvLGgtTPPv8IIUXm6dSC5YVyHMEBu8BdwHCllt2SGPQ2F/zeMt5aOPuPP7Jgq+ieTi3Ivydn8myxCEQVZy9aRRHqsTKsg2n1uO1jfoKurVlDZTqh1BFubav//Y9vH/ELnIhkb6+BL49f3M87FACw75cP3Lj9CQRi/s3fdHptSPCI6ckfUilPOkN375//83Jmm6jB3W2A0WjZepzuoXPpdXxlYKTd98Bmz01qa2v9LevYb1nH3N09Dv96GgAgFAp2ff9NfkGuTqcLHzxk8aLlAwY8WdlRXFL0/e5vS0uLKRRqfNyot9/+F4v5dFg3lUr17bYv8/JyAAAREVFLl6z08EAgNqT5UlAm0qmUqPyaJRLBjh/SFQrJlMQVkyct1eu1OzMXNTQ9iRx1NfeX1rb6N9O+npq44l7RxYtX9pnu3y48d/DoOhbDdWri+wODYusb0TqMgEgiNnayuNe+2PDJZiaTNTJhzLZvMzd8stlkoBUrF9+6XbAwfdmK5WtkS/IAAAAHH0lEQVQEwpYVKxdLZVIAQFVVxfsrF2u12g8/+OS1eenXr1/+9NNVz6b566F9586dnjlj7qKFyyQSMVL798yXggqJnoDOEpjzV/cy6C6L3thBIBABAC9EvvzltzPyb2ZNnbwCAODm6jt35qc4HM7XJ+xe8eXS8htJ4F2tVp11duuA/lHpr203xXoSCGtQciGRTFBIIUSaQ5xBA0OJRKKrKzc8fIjpzvkLZ6urq77esmto1DAAQHh41Ny0lBMnDr82P/3gLz/i8fjNm3YwGUwAAJPJ2vjlx4WFtyMjh3ZMs6Gxnkqlzp3zOpFInJw4FSmpnVhQqiOQUFlQ/fBRnkjctObzF9vv6PVakeTJMlUnJ0r7AioXjmdV9T0AQOXjQrlCNDJ+dnusMTwerRViTmSCWqlHKXG4FBbeYtAZJv8BADw8PH19/UofFQMA7hbeiooaZvIfAGDYsDgAQOmj4qcsOH7cyxcvZq9a/e47S95vr8H7Tqc+wwFUOoZSmTB0YMLkie90vEkhm2l7EQhOBoMeANAmbjQ5Eg09T2E0AoBWOxMyMrmMzXHueIfFYgsFLQAAuVzGYf/zFpPJAgAIBC1PpTA8Jv4/G7/7fve3C9JnT06cuvy91YgECDWfBI1F1GtR2axAo7LkCjHPrRdhKBh0ZwCATGGNqMs6tZ7CgL+fBik6hspw4/KKi+93fLe1VejO8wAAcLk8ieSfAdG2tlYAAOP/C8WODI+JHxYde/zEoYxd37i7e85LW9B3kea7IzQmQa9FpT4KGjCsqrqwpq6k/Y5ao+z6ES+PIBwOf7vQGgfC6NQ6GtNu1oF3DZVCFQoF7ZdhYRFSqaSkpMh0yeeX1dXVmFqKYWERdwtvtQf9zsm5CAAwvUVyIkmlTxazmUZ58Hj8rJmvcrluZX0e9zZh/i+e5UJ0IqGyqH3CmLdKHuX+cGDZqBFzmXSXh2V/GQz6N179qotHnDkeMUOT829l6XTqgUFxEqmg5FEuk4FKjDatWu/lb/cLck2Eh0ddvJT966H9TCYrLDRi/LiXf/l134bPVs1LewuPx//8cyaH4zwlZRYAIG3um5cunVv10bvJSTOamxsP/LQnakj0kMgXAACBgQPP/pG1M2PrwvR3T5w8nJt3dcL4RKGwRSBoGTgwFBGd5i3I5pJ0Kr1KqqEwER4a5Lr6LE3/4dS5bZeu7gc4nI/noBGx3Z8EO3Xy+0Qi6c69c6Xl+f6+kV4ewVIZKksK5AJZZAzkM8CQYtHCZa2tgp8PZnLYzkuWrBgwIPCrTTszdm3d9f03BoMhIjzqnSXvOzu7AAB8fHw3f7ljT+b2zV99SqXSJoxPXLxoualf+NaCd6RSSXb276/NX+jl5aPVaHZ9/w2dzpg+ffYrqfMQ0dlpZK2/zghrq4xuA5zNvuuQGI3GB+erln5jvZOIe86Of5W/tsEWhfWQunJFaYFoyttez77VadM7MJJeU95VD0ChkGz8ZprZt7guPoLW2mfvhw0aNWfGJz3T3D1KleyLr6eYfYtB45jtvoyOnzthTKctaJlQETKcjZQ8jB7SqQXdfChUmlHcJGe7m98vQqEwViz5uZOnccDcmA6JhOR2NTKJ1pkAnU5LJJpZY0GlmOnltdNS3jbtHZinkT2fdDUAMWo697/f1nVmQTwe7+Jsply1GsgKaKuTegdSnHkOvj7cBulqySrb1SlkOEPagvqJtLaAViYfPd3+IqE7AN3sHYlP4ioEMoXIvmPqdEttYcOIJBcK3XEGpe2I7nfQvbLCp/pOo1blCJP3ZqkragqLpXsHYmEV4NCjreyLNg0oy61xyLKwsaQ5dhI76sXnaOzJ1uiRBXE43JItgZK6VkmT47QLtSpdZUHtkFH0gAjHCRFhj/Qiyurslf1cXfUVN2olzfa9tF2vMzSXCZpKm1IWegyKRv3IZ4yu6V0DfESya+hwZs5JoYCvMBKcWG50+4ryIWmWK9qUbfWyhBRueII7bDkYwJL4gs480pRFno1VqrK7Mv69JjKNaDDgCCQCwYmAdyICqAfpPAsej9OqNHqNHk8ELVVyn4G0yHhGSAxmPhvCwmEIDz+Khx9l5FRua6NGLNDKJTq5WKfX6fU627IghUEgEp1oLCqdRfAJwmY+bJG+joS5eJBcPLAZBQzLgX/oA0a3GI1GT3/7HrbEE3BMF/PlHWZBOwCHw6mV+rYmO95dKqhTkWnmzYZZ0D7wC6OJW+wjOoJZ1ApdZ8vRMQvaB/FJ3Lzfm5Uyu5wmvX+9Va3Q+w82H6MC8nnEGD1HqzH8sKZi9CwPZ3cy09k+hmNbG9WPi2UalW78nE4HwjAL2hm5WS3l9+RsLqm52tan7BkcJxzeGDacFTGqq+04mAXtEo3SYPu/NhIZj+tBQw+zIAZksO4IBmQwC2JABrMgBmQwC2JABrMgBmQwC2JA5v8Aj0CaZSO8PCsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPjyj31mtlU6",
        "outputId": "0cfbc54e-d023-491a-8e1f-d0b760cd2283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<groq.resources.chat.completions.Completions object at 0x7e88cf772590> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e88cf4a1c90> model_name='llama3-8b-8192' model_kwargs={} groq_api_key=SecretStr('**********')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# Function definition\n",
        "def get_current_time_in_pakistan() -> str:\n",
        "    \"\"\"Fetches the current date and time in Pakistan Standard Time (PST).\n",
        "\n",
        "    Returns:\n",
        "        str: The current date and time in Pakistan formatted as 'YYYY-MM-DD HH:MM:SS'.\n",
        "    \"\"\"\n",
        "    pakistan_tz = pytz.timezone('Asia/Karachi')\n",
        "    current_time = datetime.now(pakistan_tz)\n",
        "    formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"get_current_time_in_pakistan() called, returning: {formatted_time}\")\n",
        "    return formatted_time\n",
        "\n",
        "# Manual test\n",
        "current_time_in_pakistan = get_current_time_in_pakistan()\n",
        "print(f\"Current time in Pakistan: {current_time_in_pakistan}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QJehTNcVyF0",
        "outputId": "ff807459-a02b-4074-f10c-7eb2eafe2396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_current_time_in_pakistan() called, returning: 2025-01-14 15:27:42\n",
            "Current time in Pakistan: 2025-01-14 15:27:42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correct tool calling"
      ],
      "metadata": {
        "id": "THDkGkbkloHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "import json\n",
        "# Initialize the chatbot state\n",
        "state = AgentState(messages=[])\n",
        "\n",
        "# Function to invoke tools and return a response through LLM\n",
        "def chatbot(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Handles user interaction by appending new messages to the state,\n",
        "    invoking tools if needed, and generating a natural language response through LLM.\n",
        "    \"\"\"\n",
        "    # Invoke the LLM with the current state\n",
        "    llm_response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "\n",
        "    # Append the LLM's response to the state\n",
        "    state['messages'].append(llm_response)\n",
        "\n",
        "    # Check if a tool was called\n",
        "    tool_calls = llm_response.additional_kwargs.get(\"tool_calls\", None)\n",
        "    if tool_calls:\n",
        "        for tool_call in tool_calls:\n",
        "            tool_name = tool_call[\"function\"][\"name\"]\n",
        "            tool_args = json.loads(tool_call[\"function\"][\"arguments\"])  # Safely parse the arguments\n",
        "\n",
        "            # Find and execute the corresponding tool\n",
        "            tool_result = None\n",
        "            for tool in tools:\n",
        "                if tool.__name__ == tool_name:  # Match tool by function name\n",
        "                    tool_result = tool(**tool_args)\n",
        "                    break\n",
        "\n",
        "            if tool_result:\n",
        "                # Add the tool result to the state and reinvoke the LLM\n",
        "                tool_result_message = HumanMessage(\n",
        "                    content=f\"The tool '{tool_name}' returned: {tool_result}\"\n",
        "                )\n",
        "                state['messages'].append(tool_result_message)\n",
        "                final_response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "                state['messages'].append(final_response)\n",
        "            else:\n",
        "                # Handle case where the tool could not be found or failed\n",
        "                error_message = HumanMessage(content=f\"Error: Tool '{tool_name}' could not be executed.\")\n",
        "                state['messages'].append(error_message)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "# Function to simulate a chatbot session\n",
        "def chatbot_test():\n",
        "    print(\"Chatbot is ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Append user input as a HumanMessage to the state\n",
        "        state['messages'].append(HumanMessage(content=user_input))\n",
        "\n",
        "        # Get the chatbot response\n",
        "        updated_state = chatbot(state)\n",
        "\n",
        "        # Extract and print the latest AI response\n",
        "        ai_response = updated_state['messages'][-1]  # The chatbot's response is the last message\n",
        "        if isinstance(ai_response, AIMessage):\n",
        "            print(f\"Chatbot: {ai_response.content}\")\n",
        "        else:\n",
        "            print(\"Chatbot: Sorry, something went wrong.\")\n",
        "\n",
        "# Start the chatbot test\n",
        "chatbot_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "PvTZJsUXln5u",
        "outputId": "c02285ac-8535-426c-bff8-1f6ff20066f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot is ready. Type 'exit' to quit.\n",
            "You: manicure\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'StructuredTool' object has no attribute '__name__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-303-12fed7cb9b42>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Start the chatbot test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mchatbot_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-303-12fed7cb9b42>\u001b[0m in \u001b[0;36mchatbot_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Get the chatbot response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mupdated_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Extract and print the latest AI response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-303-12fed7cb9b42>\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtool_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtool\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtool_name\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Match tool by function name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0mtool_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtool_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Raises AttributeError if appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'StructuredTool' object has no attribute '__name__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct Response 👍👌👌👌👌👍👍👍👍👍👍👍👍👍👍👍👍👍👍\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "import json\n",
        "# Initialize the chatbot state\n",
        "state = AgentState(messages=[])\n",
        "\n",
        "# Function to invoke tools and return a response through LLM\n",
        "def chatbot(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Handles user interaction by appending new messages to the state,\n",
        "    invoking tools if needed, and generating a natural language response through LLM.\n",
        "    \"\"\"\n",
        "    # Invoke the LLM with the current state\n",
        "    llm_response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "\n",
        "    # Append the LLM's response to the state\n",
        "    state['messages'].append(llm_response)\n",
        "\n",
        "    # Check if a tool was called\n",
        "    tool_calls = llm_response.additional_kwargs.get(\"tool_calls\", None)\n",
        "    if tool_calls:\n",
        "        for tool_call in tool_calls:\n",
        "            tool_name = tool_call[\"function\"][\"name\"]\n",
        "            tool_args = json.loads(tool_call[\"function\"][\"arguments\"])  # Safely parse the arguments\n",
        "\n",
        "            # Find and execute the corresponding tool\n",
        "            tool_result = None\n",
        "            # Iterate through the original list of tools\n",
        "            for tool in tools:  # This is the change\n",
        "                if tool.name == tool_name:\n",
        "                    # Invoke the tool using its 'run' method if available or 'func' otherwise\n",
        "                    if hasattr(tool, 'run'):  # Check if 'run' method exists\n",
        "                        tool_result = tool.run(tool_input='tool input', **tool_args)\n",
        "                    elif hasattr(tool, 'func'):  # Check if 'func' attribute exists\n",
        "                        tool_result = tool.func(**tool_args)\n",
        "                    break\n",
        "\n",
        "            if tool_result:\n",
        "                # Add the tool result to the state and reinvoke the LLM\n",
        "                tool_result_message = HumanMessage(\n",
        "                    content=f\"The tool '{tool_name}' returned: {tool_result}\"\n",
        "                )\n",
        "                state['messages'].append(tool_result_message)\n",
        "                final_response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "                state['messages'].append(final_response)\n",
        "            else:\n",
        "                # Handle case where the tool could not be found or failed\n",
        "                error_message = HumanMessage(content=f\"Error: Tool '{tool_name}' could not be executed.\")\n",
        "                state['messages'].append(error_message)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "# Function to simulate a chatbot session\n",
        "def chatbot_test():\n",
        "    print(\"Chatbot is ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Append user input as a HumanMessage to the state\n",
        "        state['messages'].append(HumanMessage(content=user_input))\n",
        "\n",
        "        # Get the chatbot response\n",
        "        updated_state = chatbot(state)\n",
        "\n",
        "        # Extract and print the latest AI response\n",
        "        ai_response = updated_state['messages'][-1]  # The chatbot's response is the last message\n",
        "        if isinstance(ai_response, AIMessage):\n",
        "            print(f\"Chatbot: {ai_response.content}\")\n",
        "        else:\n",
        "            print(\"Chatbot: Sorry, something went wrong.\")\n",
        "\n",
        "# Start the chatbot test\n",
        "chatbot_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cW6AXyEMvURP",
        "outputId": "1c70656f-bf90-4a05-e3a7-83541714f9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot is ready. Type 'exit' to quit.\n",
            "You: I want to do a manicure.\n",
            "Chatbot: A manicure sounds like a great idea!\n",
            "\n",
            "For a manicure, you can choose from a variety of options, including a basic manicure, spa manicure, or gel manicure. Here's a brief overview of each:\n",
            "\n",
            "* Basic Manicure: A classic manicure that includes nail trimming, cuticle care, and a moisturizing treatment. ($15-$25)\n",
            "* Spa Manicure: A more luxurious option that includes a warm paraffin wax treatment, exfoliating scrub, and a relaxing massage. ($30-$45)\n",
            "* Gel Manicure: A long-lasting option that uses gel polish to create a glossy finish. ($25-$40)\n",
            "\n",
            "Which type of manicure sounds appealing to you?\n",
            "You: basic manicure\n",
            "Chatbot: A basic manicure is a great option for a quick and relaxing pampering session.\n",
            "\n",
            "Here's what you can expect from a basic manicure:\n",
            "\n",
            "* Nail trimming: Your nails will be carefully trimmed to the desired length.\n",
            "* Cuticle care: The cuticles will be gently pushed back and cleaned to remove any dead skin.\n",
            "* Moisturizing treatment: A nourishing lotion or cream will be applied to hydrate your hands and cuticles.\n",
            "\n",
            "Price: $15-$20\n",
            "\n",
            "Duration: Approximately 15-30 minutes\n",
            "\n",
            "Would you like to add any extras to your basic manicure, such as a hand massage or nail polish application?\n",
            "You: I want to haircut my hairs.\n",
            "Chatbot: A haircut sounds like a great way to refresh your look!\n",
            "\n",
            "For a haircut, I can provide you with some information about different styles and options. Here are a few questions to help me better understand what you're looking for:\n",
            "\n",
            "* What's your current hair style? (e.g. short, medium, long)\n",
            "* What's your hair type? (e.g. straight, curly, wavy)\n",
            "* What's your preferred haircut style? (e.g. fade, pompadour, messy texture)\n",
            "* Do you have any specific hair concerns? (e.g. dryness, frizz, thinning)\n",
            "\n",
            "Based on your answers, I can suggest some haircut options and provide you with a quote for the service.\n",
            "\n",
            "Price: $20-$50\n",
            "\n",
            "Duration: Approximately 30-60 minutes\n",
            "\n",
            "Would you like to schedule an appointment for a haircut?\n",
            "You: My current hair style is short and it's straight and my prefered haircut is fade \n",
            "Chatbot: A short, straight haircut with a fade sounds like a great look!\n",
            "\n",
            "For a haircut, I can provide you with a quote and some information about the service. Here's what you can expect:\n",
            "\n",
            "* A haircut with a fade typically includes a trim, shaping, and blending of the hair to create a clean, defined edge.\n",
            "* The price for a haircut with a fade can vary depending on the length and thickness of your hair, as well as the complexity of the design. On average, a haircut with a fade can cost between $20-$50.\n",
            "* The duration of the service can also vary depending on the details of the haircut. On average, a haircut with a fade can take around 30-60 minutes.\n",
            "\n",
            "Here's a tool call object to help me provide you with a more accurate quote:\n",
            "\n",
            "{\n",
            "\"tool_calls\": [\n",
            "{\n",
            "\"id\": \"pending\",\n",
            "\"type\": \"function\",\n",
            "\"function\": {\n",
            "\"name\": \"haircut_service\"\n",
            "},\n",
            "\"parameters\": {\n",
            "\"style\": \"fade\",\n",
            "\"length\": \"short\"\n",
            "}\n",
            "}\n",
            "]\n",
            "}\n",
            "\n",
            "Please let me know if this sounds good to you, and I'll provide you with a more detailed quote and scheduling information.\n",
            "You: OK.\n",
            "haircut_service(style='tool input') called, returning: {'price': '1500 PKR', 'time': '30 minutes'}\n",
            "Chatbot: The 'haircut_service' tool has provided the price and time estimate for the haircut.\n",
            "\n",
            "Based on the output, the total cost for the haircut with a fade would be 1500 PKR, and the service would take approximately 30 minutes.\n",
            "\n",
            "Here's an updated response:\n",
            "\n",
            "\"Great! The haircut with a fade will cost 1500 PKR and take around 30 minutes. Would you like to proceed with the service?\"\n",
            "\n",
            "Please let me know if you'd like to confirm the service or make any changes.\n",
            "You: OK now how much total cost i have and how much time needed for these services?\n",
            "Chatbot: Let's recap the services we've discussed so far:\n",
            "\n",
            "1. Basic Manicure: $15-$20\n",
            "2. Haircut with Fade: $1500 PKR (approximately $9.50 USD)\n",
            "\n",
            "Total Cost: $15-$20 + $9.50 = $24.50-$29.50\n",
            "\n",
            "Time Needed:\n",
            "\n",
            "1. Basic Manicure: 15-30 minutes\n",
            "2. Haircut with Fade: 30 minutes\n",
            "\n",
            "Total Time Needed: 45-60 minutes\n",
            "\n",
            "So, the total cost for the services would be around $24.50-$29.50, and the total time needed would be around 45-60 minutes.\n",
            "\n",
            "Please note that the prices and time estimates may vary depending on the location, salon, and other factors.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-310-d3d8a6a87372>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Start the chatbot test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mchatbot_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-310-d3d8a6a87372>\u001b[0m in \u001b[0;36mchatbot_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot is ready. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "# Initialize the chatbot state\n",
        "state = AgentState(messages=[])\n",
        "\n",
        "# Function to invoke tools and return a response through LLM\n",
        "def chatbot(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Handles user interaction by appending new messages to the state,\n",
        "    invoking tools if needed, and generating a natural language response through LLM.\n",
        "    \"\"\"\n",
        "    # Invoke the LLM with the current state\n",
        "    llm_response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "\n",
        "    # Check if a tool was called\n",
        "    tool_calls = llm_response.additional_kwargs.get(\"function_call\", None)\n",
        "    if tool_calls:\n",
        "        # Parse tool call details\n",
        "        tool_name = tool_calls[\"name\"]\n",
        "        tool_args = tool_calls[\"arguments\"]\n",
        "        tool_args = eval(tool_args)  # Convert arguments from JSON string to dictionary\n",
        "\n",
        "        # Execute the tool\n",
        "        tool_result = None\n",
        "        for tool in tools:\n",
        "            if tool.name == tool_name:\n",
        "                tool_result = tool.func(**tool_args)\n",
        "                break\n",
        "\n",
        "        # Create a message with the tool result and reinvoke the LLM\n",
        "        tool_result_message = HumanMessage(\n",
        "            content=f\"The tool '{tool_name}' calculated the result: {tool_result}. \"\n",
        "                    f\"Please craft a natural language response for the user.\"\n",
        "        )\n",
        "        final_response = llm_with_tools.invoke([sys_msg] + state['messages'] + [tool_result_message])\n",
        "\n",
        "        # Append the final response to the state\n",
        "        state['messages'].append(final_response)\n",
        "    else:\n",
        "        # If no tool was called, append the original response\n",
        "        state['messages'].append(llm_response)\n",
        "\n",
        "    return state\n",
        "\n",
        "# Function to simulate a chatbot session\n",
        "import time\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# Function to simulate a chatbot session\n",
        "def chatbot_test():\n",
        "    print(\"Chatbot is ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Append user input as a HumanMessage to the state\n",
        "        state['messages'].append(HumanMessage(content=user_input))\n",
        "\n",
        "        # Get the chatbot response\n",
        "        updated_state = chatbot(state)\n",
        "\n",
        "        # Extract and print the latest AI response\n",
        "        ai_response = updated_state['messages'][-1]  # The chatbot's response is the last message\n",
        "        if isinstance(ai_response, AIMessage):\n",
        "            print(f\"Chatbot: {ai_response.content}\")\n",
        "        else:\n",
        "            print(\"Chatbot: Sorry, something went wrong.\")\n",
        "\n",
        "        # Introduce a delay (e.g., 1 second) to avoid exceeding rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "# Start the chatbot test\n",
        "chatbot_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "KZ3EA2RDlnsx",
        "outputId": "2b23fc82-d691-40bf-a70c-636c720c96ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot is ready. Type 'exit' to quit.\n",
            "You: manicure\n",
            "Chatbot: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-296-511100cfbd2c>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Start the chatbot test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mchatbot_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-296-511100cfbd2c>\u001b[0m in \u001b[0;36mchatbot_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot is ready. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorrect tool calling"
      ],
      "metadata": {
        "id": "fBqpI62MlyM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tool_request(user_input):\n",
        "    \"\"\"\n",
        "    Check if the user input matches a tool request and extract the tool name and arguments.\n",
        "    \"\"\"\n",
        "    if \"current time\" in user_input.lower() or \"current date\" in user_input.lower():\n",
        "        return \"get_current_time_in_pakistan\", []\n",
        "    try:\n",
        "        parts = user_input.strip().split()\n",
        "        if len(parts) >= 3:\n",
        "            tool_name = parts[0].lower()\n",
        "            args = list(map(float, parts[1:]))\n",
        "            if tool_name in ['add', 'subtract', 'multiply']:\n",
        "                return tool_name, args\n",
        "    except ValueError:\n",
        "        pass\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def chatbot(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Handles user interaction, checking if a tool is invoked and calling it if necessary.\n",
        "    \"\"\"\n",
        "    # Get the latest user message\n",
        "    user_message = state['messages'][-1]\n",
        "\n",
        "    # Check if the user message triggers a tool\n",
        "    tool_name, tool_args = parse_tool_request(user_message.content)\n",
        "    if tool_name:\n",
        "        # Find and invoke the tool\n",
        "        tool_result = None\n",
        "        for tool in tools:\n",
        "            if tool.name == tool_name:\n",
        "                tool_result = tool.func(*tool_args)\n",
        "                break\n",
        "\n",
        "        # Create a response with the tool result\n",
        "        tool_result_message = AIMessage(\n",
        "            content=f\"The tool '{tool_name}' calculated the result: {tool_result}.\"\n",
        "        )\n",
        "        state['messages'].append(tool_result_message)\n",
        "    else:\n",
        "        # Standard LLM invocation if no tool is needed\n",
        "        llm_response = llm_with_tools.invoke([sys_msg] + state['messages'])\n",
        "        state['messages'].append(llm_response)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "# Function to simulate a chatbot session\n",
        "def chatbot_test():\n",
        "    print(\"Chatbot is ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Append user input as a HumanMessage to the state\n",
        "        state['messages'].append(HumanMessage(content=user_input))\n",
        "\n",
        "        # Get the chatbot response\n",
        "        updated_state = chatbot(state)\n",
        "\n",
        "        # Extract and print the latest AI response\n",
        "        ai_response = updated_state['messages'][-1]\n",
        "        if isinstance(ai_response, AIMessage):\n",
        "            print(f\"Chatbot: {ai_response.content}\")\n",
        "        else:\n",
        "            print(\"Chatbot: Sorry, something went wrong.\")\n",
        "\n",
        "\n",
        "# Start the chatbot test\n",
        "chatbot_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "kbKPEbdLVb8P",
        "outputId": "6c200afc-8218-48aa-afb2-750cfad02659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot is ready. Type 'exit' to quit.\n",
            "You: manicure\n",
            "Chatbot: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-275-80af947bfeb4>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Start the chatbot test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mchatbot_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-275-80af947bfeb4>\u001b[0m in \u001b[0;36mchatbot_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot is ready. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DRkZtDQ5Vb5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pmJAyOTCVb3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XAz2nnI8Vb08"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}